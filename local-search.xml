<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>CMU 15-445 (fall 2023) Project 1 BUFFER POOL</title>
    <link href="/2024/03/16/CMU-15-445-645-fall-2023-Project-1-BUFFER-POOL/"/>
    <url>/2024/03/16/CMU-15-445-645-fall-2023-Project-1-BUFFER-POOL/</url>
    
    <content type="html"><![CDATA[<p>本篇为15-445 project 1 buffer pool的总结。</p><span id="more"></span><h2 id="写在开始之前"><a href="#写在开始之前" class="headerlink" title="写在开始之前"></a>写在开始之前</h2><p>看15-445的网课加做lab断断续续持续了三个月，中间历经申请硕士写文书的癫狂，过年休息的惬意，15-445一直作为副线穿插在我的日常生活中，但每次写完一个15-445的project，我都有一种身心的愉悦，有一种我变秃了也变强的快乐，也在此感谢Andy和CMU DB group能够把这门神课开放给所有人，正是经过了这门课的洗礼让我从一个对数据库一无所知的小白成长为对数据库每个组件都略知一二的入门者，享受在数据库中遨游的快乐。</p><p>在此写下我对lab的一些总结，由于有不少大佬已经写了很不错的经验贴，我应该不会描述lab中过多的细节，权当对lab的复盘，想到的一些相关的问题也会做较多的发散，希望能对我找到相关的实习有所帮助（。</p><h2 id="关于debug"><a href="#关于debug" class="headerlink" title="关于debug"></a>关于debug</h2><p>写bustub时是免不了调试的，我并不会gdb这类学习成本比较高的调试工具，但是仅靠IDE提供的调试工具和在代码中print相关信息也顺利通过了P1-P4。输出信息时可以善用bustub内置的宏，eg. <code>LOG_INFO(&quot;Writing page %d to disk, data: %s&quot;, r.page_id_, r.data_);</code>这可以打印出较为详细的信息。同时大模型时代也请善用AI，学生能免费用github copilot实在太爽了。</p><h2 id="什么是buffer-pool"><a href="#什么是buffer-pool" class="headerlink" title="什么是buffer pool?"></a>什么是buffer pool?</h2><p>虽然数据库的数据最终都存储在磁盘当中，但当我们读取和修改数据时，都需要在内存中进行，而buffer pool解决的就是取什么，什么时候取，如果内存满了将哪些数据写回磁盘，怎样写回磁盘的问题。相较于让操作系统代替我们管理内存，由于我们知道每次操作的语义，自己对内存进行管理能够保证数据库以更高的效率运行。而我们的buffer pool通过将内存划分为一个个大小与page相同的frame，通过frame容纳的page的切换达成数据的读入与写出。这里可以看看小林coding上对mysql中buffer pool的<a href="https://xiaolincoding.com/mysql/buffer_pool/buffer_pool.html">描述</a>。</p><h2 id="LRU-K-replacer"><a href="#LRU-K-replacer" class="headerlink" title="LRU-K replacer"></a>LRU-K replacer</h2><p>Replacer负责的就是当内存满时驱逐哪个page将要读的page置换进来，决策的越好，就能保证数据在内存中的命中率越高，从而提升数据库的性能（这里需要了解寄存器，cpu cache, 内存, 磁盘的速度差异）。<img src="/../img/assets/image-20240316234337766.png" alt="不同存储器速度差异"></p><p>这里唯一需要注意的是Evict时驱逐 evictable 且具有最大 k-distance 的 frame，而对于未满 k 次访问的 frame， 需要比较第一次访问的时间戳，而<strong>不是最近一次的</strong>。我在驱逐时偷懒直接用优先队列把所有Node都存了一遍，然后把满足条件的第一个Node踢掉。之前看到过<a href="https://www.bilibili.com/video/BV16u4m1c7cU/">一个视频</a>说过对于较小的数据量，复杂度较高的算法并不是不可接受的，对于理论上复杂度更优的算法，即使看上去优化了一个数量级，但常数项对于较小的数据量并不可忽略，而当我们调用封装好并且优化过的算法时反而能以更快的速度和更简单的代码完成任务（给自己偷懒找理由来了）。</p><p>这里能想到的可能在面试里会遇到的问题应该就是<a href="https://leetcode.cn/problems/lru-cache/description/">手撕LRU算法</a>，熟悉一下双向链表即可。</p><h2 id="Disk-scheduler"><a href="#Disk-scheduler" class="headerlink" title="Disk scheduler"></a>Disk scheduler</h2><p>这里是23fall唯一需要我们亲自写多线程的代码，但并不需要特别了解多线程，而channel和promise对于写过golang和前端的同学来说应该也比较熟悉，语法看一看cppreference也就懂了。写到这里也可以去看看Disk Manager是怎么实现的，由于page的大小固定，所以我们只要通过page_id * PAGE_SIZE就可以获得page的offset，也就能找到相应的文件了。</p><h2 id="Buffer-pool-manager"><a href="#Buffer-pool-manager" class="headerlink" title="Buffer pool manager"></a>Buffer pool manager</h2><p>P1的重点是理解<code>pin_count</code>，类似于引用计数的概念，只有当<code>pin_count</code>为0时才能保证该page当前没有被访问，才能安全地对它进行刷回磁盘等操作，而迟先生为我们引入的PageGuard也给之后page信息的维护省了不少精力。关于并发控制，我也是全程拿了一把大锁先保证能成功实现项目，之后有空的话再做优化和leaderboard task了。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>P1 buffer pool应该是四个项目中最简单的project了，但是对于刚开始不熟悉C++的我来说也着实废了一番力气。虽然简单，但后面的索引，查询执行都依赖于buffer pool的实现，buffer pool实现的正确性关切到之后project能不能正常的运行，还是需要认真检查有没有出纰漏的。</p><p>P2 Hash Index的总结应该会在下周写完，希望能做到周更（。</p>]]></content>
    
    
    
    <tags>
      
      <tag>15-445</tag>
      
      <tag>database</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIT 6.5840(6.824) | ZooKeeper Note</title>
    <link href="/2023/08/31/MIT-6-5840-6-824-ZooKeeper-Note/"/>
    <url>/2023/08/31/MIT-6-5840-6-824-ZooKeeper-Note/</url>
    
    <content type="html"><![CDATA[<p>本篇文章为ZooKeeper的学习笔记。</p><span id="more"></span><p>Zookeeper可以被认为是一个通用的协调服务（General-Purpose Coordination Service），通过多副本来完成容错。</p><h2 id="性能问题"><a href="#性能问题" class="headerlink" title="性能问题"></a>性能问题</h2><p>如果一个读写都通过leader的多副本系统添加服务器，并不能保证性能的提升，相反leader压力增大，性能下降。但如果只将写请求交由leader处理，读请求可以由任一个副本处理，多服务器可以让性能提升。</p><p>问题：除了Leader以外的任何一个副本数据不一定是最新（up to date）的：副本不在leader所处的过半服务器中，或者已经完全失联了。</p><p>解决方式：ZooKeeper并不要求返回最新的写入数据。Zookeeper的确允许客户端将读请求发送给任意副本，并由副本根据自己的状态来响应读请求。副本的Log可能并没有拥有最新的条目，所以尽管系统中可能有一些更新的数据，这个副本可能还是会返回旧的数据（不提供强一致性）。</p><h2 id="一致保证"><a href="#一致保证" class="headerlink" title="一致保证"></a>一致保证</h2><ol><li><p>写请求线性一致</p></li><li><p>任何一个客户端的请求，都会按照客户端指定的顺序来执行，论文里称之为FIFO（First In First Out）客户端序列。</p><ol><li><p>为了让Leader可以实际的按照客户端确定的顺序执行写请求，我设想，客户端实际上会对它的写请求打上序号，表明它先执行这个，再执行这个，第三个是这个，而Zookeeper Leader节点会遵从这个顺序。这里由于有这些异步的写请求变得非常有意思。</p></li><li><p>对于读请求，其不需要经过Leader，只有写请求经过Leader，读请求只会到达某个副本。所以，读请求只能看到那个副本的Log对应的状态。对于读请求，我们应该这么考虑FIFO客户端序列，客户端会以某种顺序读某个数据，之后读第二个数据，之后是第三个数据，对于那个副本上的Log来说，每一个读请求必然要在Log的某个特定的点执行，或者说每个读请求都可以在Log一个特定的点观察到对应的状态。然后，后续的读请求，必须要在不早于当前读请求对应的Log点执行。也就是一个客户端发起了两个读请求，如果第一个读请求在Log中的一个位置执行，那么第二个读请求只允许在第一个读请求对应的位置或者更后的位置执行。<strong>第二个读请求不允许看到之前的状态，第二个读请求至少要看到第一个读请求的状态。这是一个极其重要的事实。</strong></p></li><li><p>若之前处理读请求的副本发生故障，新副本处理切换来的读请求时，必须在原副本处理上一个请求的log点位或其之后进行。<br>此机制的工作原理是，每个Log条目都会被Leader打上zxid的标签，这些标签就是Log对应的条目号。任何时候一个副本回复一个客户端的读请求，首先这个读请求是在Log的某个特定点执行的，其次回复里面会带上zxid，对应的就是Log中执行点的前一条Log条目号。客户端会记住最高的zxid，当客户端发出一个请求到一个相同或者不同的副本时，它会在它的请求中带上这个最高的zxid。这样，其他的副本就知道，应该至少在Log中这个点或者之后执行这个读请求。这里有个有趣的场景，如果第二个副本并没有最新的Log，当它从客户端收到一个请求，客户端说，上一次我的读请求在其他副本Log的这个位置执行，在获取到对应这个位置的Log之前，这个副本不能响应客户端请求。</p></li><li><p>FIFO客户端请求序列是对一个客户端的所有读请求，写请求生效。所以，如果我发送一个写请求给Leader，在Leader commit这个请求之前需要消耗一些时间，所以我现在给Leader发了一个写请求，而Leader还没有处理完它，或者commit它。之后，我发送了一个读请求给某个副本。这个读请求需要暂缓一下，以确保FIFO客户端请求序列。读请求需要暂缓，直到这个副本发现之前的写请求已经执行了。这是FIFO客户端请求序列的必然结果，（对于某个特定的客户端）读写请求是线性一致的。</p><p>最明显的理解这种行为的方式是，如果一个客户端写了一份数据，例如向Leader发送了一个写请求，之后立即读同一份数据，并将读请求发送给了某一个副本，那么客户端需要看到自己刚刚写入的值。如果我写了某个变量为17，那么我之后读这个变量，返回的不是17，这会很奇怪，这表明系统并没有执行我的请求。因为如果执行了的话，写请求应该在读请求之前执行。所以，副本必然有一些有意思的行为来暂缓客户端，比如当客户端发送一个读请求说，我上一次发送给Leader的写请求对应了zxid是多少，这个副本必须等到自己看到对应zxid的写请求再执行读请求。</p></li></ol></li></ol><p class="note note-info">从一个副本读取的或许不是最新的数据，所以Leader或许已经向过半服务器发送了C，并commit了，过半服务器也执行了这个请求。但是这个副本并不在Leader的过半服务器中，所以或许这个副本没有最新的数据。这就是Zookeeper的工作方式，它并不保证我们可以看到最新的数据。Zookeeper可以保证读写有序，但是只针对一个客户端来说。所以，如果我发送了一个写请求，之后我读取相同的数据，Zookeeper系统可以保证读请求可以读到我之前写入的数据。但是，如果你发送了一个写请求，之后我读取相同的数据，并没有保证说我可以看到你写入的数据。这就是Zookeeper可以根据副本的数量加速读请求的基础。</p><h2 id="同步操作"><a href="#同步操作" class="headerlink" title="同步操作"></a>同步操作</h2><p>同步操作可以保证读到最新的数据，用于弥补非严格线性一致。</p><p>Zookeeper有一个操作类型是sync，它本质上就是一个写请求。假设我知道你最近写了一些数据，并且我想读出你写入的数据，所以现在的场景是，我想读出Zookeeper中最新的数据。这个时候，我可以发送一个sync请求，它的效果相当于一个写请求，所以它最终会出现在所有副本的Log中，尽管我只关心与我交互的副本，因为我需要从那个副本读出数据。接下来，在发送读请求时，我（客户端）告诉副本，在看到我上一次sync请求之前，不要返回我的读请求。</p><p>如果这里把sync看成是一个写请求，这里实际上符合了FIFO客户端请求序列，因为读请求必须至少要看到同一个客户端前一个写请求对应的状态。所以，如果我发送了一个sync请求之后，又发送了一个读请求。Zookeeper必须要向我返回至少是我发送的sync请求对应的状态。</p><p>不管怎么样，如果我需要读最新的数据，我需要发送一个sync请求，之后再发送读请求。这个读请求可以保证看到sync对应的状态，所以可以合理的认为是最新的。但是同时也要认识到，这是一个代价很高的操作，因为我们现在将一个廉价的读操作转换成了一个耗费Leader时间的sync操作。所以，如果不是必须的，那还是不要这么做。</p><h2 id="就绪文件"><a href="#就绪文件" class="headerlink" title="就绪文件"></a>就绪文件</h2><p>To be updated</p>]]></content>
    
    
    
    <tags>
      
      <tag>MIT6.5840</tag>
      
      <tag>分布式系统</tag>
      
      <tag>Note</tag>
      
      <tag>ZooKeeper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIT 6.5840(6.824) | Lab2: Raft</title>
    <link href="/2023/08/22/MIT-6-5840-6-824-Lab2-Raft/"/>
    <url>/2023/08/22/MIT-6-5840-6-824-Lab2-Raft/</url>
    
    <content type="html"><![CDATA[<p>本篇文章主要讲述学习raft的road map与我自己踩的一些坑。</p><span id="more"></span><p>网络上关于raft的讲解实在是太多了，我作为一名初学者很难做到比各位大佬讲解的更好，因此打算对现有的资料进行一些整合，帮助初学者更快的上手raft。</p><h2 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h2><p>在开始之前，你至少应该略读过<a href="https://pdos.csail.mit.edu/6.824/papers/raft-extended.pdf">raft论文</a>，对figure 2有基本的认识，了解文中每个section具体在解决什么问题，以防之后debug时无从下手。</p><p><img src="https://user-images.githubusercontent.com/32640567/116203223-0bbb5680-a76e-11eb-8ccd-4ef3f1006fb3.png" alt="figure 2"></p><p>同时，你应该看过6.824的raft课堂回放，Robert Morris 教授的讲解十分清晰，这里推荐肖宏辉大佬整理的<a href="https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/">图文翻译版课程内容</a>方便随时回看。</p><p>现在，你应该对raft有大概的认识了，如果还没有的话，推荐这个<a href="http://thesecretlivesofdata.com/raft/">可视化raft的网站</a>，可以帮助你快速的建立起对raft的认知。助教提供的<a href="https://pdos.csail.mit.edu/6.824/notes/raft_diagram.pdf">diagram</a>也可以帮助你理解raft的内部机制。</p><p><img src="/img/assets/image-20230822233439166.png" alt="raft diagram"></p><h2 id="开始编码"><a href="#开始编码" class="headerlink" title="开始编码"></a>开始编码</h2><p>尽管lab页面提供了大量关于结构设计与锁的相关instructions，但是作为初学者在没有真正熟悉Go与raft的机制之前很难理解其中的具体内容。这里我推荐先参考助教的<a href="https://blog.josejg.com/debugging-pretty/">debug博客</a>将log工具配置好。相信我，你一定会用到它的（。</p><h3 id="Leader-election"><a href="#Leader-election" class="headerlink" title="Leader election"></a>Leader election</h3><p>经过以上内容的学习，你应该可以做到在脑海里可视化raft的机制并开启lab 2A的编码工作了。Leader选举并不涉及到log的添加，这里所有有关于log的逻辑都可以暂不实现，包括但不限于含nextIndex, matchIndex, logIndex, logTerm的逻辑。</p><p>一开始我想通过一个类似于如下的函数实现有限自动机，进行统一的follower, candidate与leader状态转换，但实际上很难实现，这样的方式很难实现并发编程与锁的获取与释放，遂而放弃。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-params">(rf *Raft)</span></span> EmitStateTransfer(event RfEvent) &#123;<br><span class="hljs-keyword">switch</span> event &#123;<br><span class="hljs-keyword">case</span> ReceiveHeartbeat:<br><span class="hljs-keyword">if</span> rf.rfState == CANDIDATE &#123;<br>rf.transferCandidateToFollower()<br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> rf.rfState == FOLLOWER &#123;<br>rf.resetHeartbeatTimer()<br>&#125;<br><span class="hljs-keyword">case</span> HeratbeatTimeout:<br><span class="hljs-keyword">if</span> rf.rfState == FOLLOWER &#123;<br>rf.transferFollowerToCandidate()<br>&#125;<br><span class="hljs-keyword">case</span> ElectionTimeout:<br><span class="hljs-keyword">if</span> rf.rfState == CANDIDATE &#123;<br>rf.resetElectionTimer()<br>&#125;<br><span class="hljs-keyword">case</span> ReceiveMajorityVote:<br><span class="hljs-keyword">if</span> rf.rfState == CANDIDATE &#123;<br>rf.transferCandidateToLeader()<br>&#125;<br><span class="hljs-keyword">case</span> CurrTermLower:<br><span class="hljs-keyword">if</span> rf.rfState == CANDIDATE &#123;<br>rf.transferCandidateToFollower()<br>&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> rf.rfState == LEADER &#123;<br>rf.transferLeaderToFollower()<br>&#125;<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>同时，我这里还踩的一个大坑就是没有分清楚electionTimeOut与heartbeatTimeOut。heartbeatTimeOut只对当前的leader作用，发生heartbeat超时就发送一轮空entries来保证follower的electionTimer不会超时。</p><p>关于锁，如果你不确定的话，一切读取与修改raft节点状态的代码加上一把锁即可保证平安，这时再读lab提供的<a href="https://pdos.csail.mit.edu/6.824/labs/raft-locking.txt">关于锁的instructions</a>会更加清楚。</p><p>如果你能仔细的根据figure 2来编写代码的话，相信leader election难不倒你。在开始lab2B之前，建议使用-race参数进行多次测试保证leader选举时没有数据竞争的发生。</p><h3 id="Log-replication"><a href="#Log-replication" class="headerlink" title="Log replication"></a>Log replication</h3><p>日志复制是raft细节最多的地方，不注意就会出现bug，我在这里花费了大量时间进行调试。</p><p>这里我踩的第一个坑是log的初始index，我没有注意到figure 2中说明了log的初始index为1，花费了大量时间研究测试代码和调试才发现问题。</p><p>另一个大坑是appendEntries应该与reply的处理在同一个goroutine中进行，否则会因为有大量的rpc触发导致处理reply的进程抢不到锁，进而无法更新nextIndex与commitIndex，raft无法顺利推进下去。</p><p>同时你也可以参考<a href="https://www.sofastack.tech/blog/sofa-jraft-pipeline-principle/">SOFAJRaft 日志复制</a>为每一个raft节点都开一个replicator goroutine来批量异步的进行日志复制，这可以大大减少rpc的调用与重复信息的发送。</p><h3 id="Persistence"><a href="#Persistence" class="headerlink" title="Persistence"></a>Persistence</h3><p>待更新</p><h3 id="Log-compaction"><a href="#Log-compaction" class="headerlink" title="Log compaction"></a>Log compaction</h3><p>待更新</p><h3 id="测试时间优化"><a href="#测试时间优化" class="headerlink" title="测试时间优化"></a>测试时间优化</h3><p>你可以通过尝试调整electionTimeOut与heartbeatTimeOut的时间来加快测试进度。同时也可以使用助教提供的可视化测试脚本同时运行大量测试用例。</p><h2 id="遇到瓶颈？"><a href="#遇到瓶颈？" class="headerlink" title="遇到瓶颈？"></a>遇到瓶颈？</h2><p>推荐参考<a href="https://github.com/OneSizeFitsQuorum">OneSizeFitsQuorum</a>大佬的<a href="https://github.com/OneSizeFitsQuorum/MIT6.824-2021/blob/master/docs/lab2.md">lab文档</a>，相信你一定能受到启发。</p>]]></content>
    
    
    
    <tags>
      
      <tag>MIT6.5840</tag>
      
      <tag>分布式系统</tag>
      
      <tag>Raft</tag>
      
      <tag>共识算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>为开源项目持续贡献的最佳实践</title>
    <link href="/2023/08/09/%E4%B8%BA%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E6%8C%81%E7%BB%AD%E8%B4%A1%E7%8C%AE%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
    <url>/2023/08/09/%E4%B8%BA%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E6%8C%81%E7%BB%AD%E8%B4%A1%E7%8C%AE%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
    
    <content type="html"><![CDATA[<p>本篇文章主要讲述为开源项目做贡献的流程与仓库管理方法，希望能够为刚刚接触开源的同学节省时间与经历。同时，本流程也适用于团队之间的协作。</p><span id="more"></span><p>这个暑假我通过OSPP接触到了Pycasbin社区，也成为了Pycasbin的maintainer，在刚开始提交pr的时候走了不少弯路，由此写下这篇文章希望能够帮到阅读到本篇文章的你。</p><h2 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h2><p>在开始之前，你应该对git与github有初步的了解，同时，你也可以安装<a href="https://git-fork.com/">Fork</a>，它是一个可视化操作git的软件，对于处理一些复杂的情况十分有用。</p><h2 id="贡献流程"><a href="#贡献流程" class="headerlink" title="贡献流程"></a>贡献流程</h2><h3 id="fork与新建branch"><a href="#fork与新建branch" class="headerlink" title="fork与新建branch"></a>fork与新建branch</h3><p>当你决定好要为项目做出贡献时，首先你应该fork一个自己的仓库，方便对其进行修改，fork在github项目页面的右上角。</p><p><img src="/img/assets/image-20230809112326623.png" alt="fork项目"></p><p>完成之后，页面会自动跳转到你属于你自己的fork仓库，这时将代码clone到本地。同时为其新建一个branch。如果你想要为项目贡献的是一个新的feature的话，建议将branch命名为<code>feat/your-feature</code>，如果是修复bug，可以命名为<code>fix/bug-to-be-fixed</code>。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone git@github.com:BustDot/django-authorization.git<br>git checkout -b &quot;feat/django-auth&quot;<br></code></pre></td></tr></table></figure><p class="note note-info">如果你只是打算做一次性贡献，可以直接在fork仓库的master分支下进行修改。若打算进行持续贡献，良好的分支管理可以为之后的工作省去很多麻烦。</p><p class="note note-info">fork的项目默认不会开启github action，如果项目有配置github workflow，需要你在fork仓库的action tab下手动打开。我会建议你开启action，这样可以保证在提交pr前可以通过大部分check，减少后续沟通成本。</p><h3 id="发起pull-request"><a href="#发起pull-request" class="headerlink" title="发起pull request"></a>发起pull request</h3><p>当完成代码的修改并push到远程仓库后，下一步即发起pull request. 在发起pr前，你需要检查当前仓库是否与源仓库保持一致，如果此时源仓库有新的commit时，需要同步当前的fork仓库，点击<code>sync fork</code>并按照提示操作即可。保证与源仓库同步后，点击<code>contribute</code>即可发起pr</p><p><img src="/img/assets/image-20230816120441774.png" alt="发起pr"></p><p>填写相应的pr信息并发起pr。标题请遵循你需要贡献的仓库规范。主体内容阐述清楚进行了哪些修改。</p><p><img src="/img/assets/image-20230816120741721.png" alt="填写pr信息"></p><p>至此，在pr页面确保自己的代码通过了所有检查，等待owner review与并入主仓库即可！</p><h2 id="Some-tips"><a href="#Some-tips" class="headerlink" title="Some tips"></a>Some tips</h2><h3 id="如何修改上一次的commit？"><a href="#如何修改上一次的commit？" class="headerlink" title="如何修改上一次的commit？"></a>如何修改上一次的commit？</h3><p>我们经常会遇到需要对已经提交的代码进行一些微小的改动（尤其常见于格式规范，如为py文件的末尾添加空行等），这时多添加一条commit信息显得有些多余，git amend可以在这里派上用场。使用amend可以修改上一次commit的信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git add .<br>git commit --amend -m &quot;updated commit message&quot;<br></code></pre></td></tr></table></figure><p>如果你已经把上一次commit push到远程仓库，你可以使用<code>git push --force-with-lease</code>来强制修改远程仓库。注意，如果你使用jetbrain系列IDE并勾选Amend进行commit and push时，一般会提示你权限不足。正确的操作应该是在对本地仓库amend commit并在命令行中进行强制push。</p><p class="note note-danger">你只应该对还未合并到master分支且只有自己负责的branch进行amend操作，否则这会打乱历史commit信息并给其他合作者带来困扰。</p><h3 id="我如何对已经提交pr的代码进行修改？"><a href="#我如何对已经提交pr的代码进行修改？" class="headerlink" title="我如何对已经提交pr的代码进行修改？"></a>我如何对已经提交pr的代码进行修改？</h3><p>按照正常流程push新commit即可，pr会自动更新新提交的代码。</p>]]></content>
    
    
    
    <tags>
      
      <tag>github</tag>
      
      <tag>open source</tag>
      
      <tag>casbin</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIT 6.5840(6.824) | GFS Note</title>
    <link href="/2023/08/08/MIT-6-5840-6-824-GFS-Note/"/>
    <url>/2023/08/08/MIT-6-5840-6-824-GFS-Note/</url>
    
    <content type="html"><![CDATA[<p>本篇文章为GFS的学习笔记。</p><span id="more"></span><h2 id="分布式系统的难点"><a href="#分布式系统的难点" class="headerlink" title="分布式系统的难点"></a>分布式系统的难点</h2><p>CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性）这3个基本需求，最多只能同时满足其中的2个。</p><h2 id="GFS-master"><a href="#GFS-master" class="headerlink" title="GFS master"></a>GFS master</h2><p>Master为Active-Standby模式，所以只有一个Master节点在工作。Master节点保存了文件名和存储位置的对应关系。</p><p>Master节点主要存储两个表单：</p><p>第一个是文件名到Chunk ID或者Chunk Handle数组的对应。这个表单告诉你，文件对应了哪些Chunk。但是只有Chunk ID是做不了太多事情的，所以有了第二个表单。</p><p>第二个表单记录了Chunk ID到Chunk数据的对应关系。这里的数据又包括了：</p><ul><li>每个Chunk存储在哪些服务器上，所以这部分是Chunk服务器的列表</li><li>每个Chunk当前的版本号，所以Master节点必须记住每个Chunk对应的版本号。</li><li>所有对于Chunk的写操作都必须在主Chunk（Primary Chunk）上顺序处理，主Chunk是Chunk的多个副本之一。所以，Master节点必须记住哪个Chunk服务器持有主Chunk。</li><li>并且，主Chunk只能在特定的租约时间内担任主Chunk，所以，Master节点要记住主Chunk的租约过期时间。</li></ul><p>为了保证master节点重启后不丢失，一部分数据需要写入log。Master节点读数据只会从内存读，但是写数据的时候，至少有一部分数据会接入到磁盘中。更具体来说，Master会在磁盘上存储log，每次有数据变更时，Master会在磁盘的log中追加一条记录，并生成CheckPoint（类似于备份点）。</p><p class="note note-info">为什么在磁盘中维护log而不是数据库？<br><br>数据库本质上来说是某种B树（b-tree）或者hash table，相比之下，追加log会非常的高效，因为你可以将最近的多个log记录一次性的写入磁盘。因为这些数据都是向同一个地址追加，这样只需要等待磁盘的磁碟旋转一次。而对于B树来说，每一份数据都需要在磁盘中随机找个位置写入。所以使用Log可以使得磁盘写入更快一些。</p><p>有些数据需要存在磁盘上，而有些不用。它们分别是：</p><ul><li>Chunk Handle的数组（第一个表单）要保存在磁盘上。我给它标记成NV（non-volatile, 非易失），这个标记表示对应的数据会写入到磁盘上。</li><li>Chunk服务器列表不用保存到磁盘上。因为Master节点重启之后可以与所有的Chunk服务器通信，并查询每个Chunk服务器存储了哪些Chunk，所以我认为它不用写入磁盘。所以这里标记成V（volatile），</li><li>版本号要不要写入磁盘取决于GFS是如何工作的，我认为它需要写入磁盘。我们之后在讨论系统是如何工作的时候再详细讨论这个问题。这里先标记成NV。</li><li>主Chunk的ID，几乎可以确定不用写入磁盘，因为Master节点重启之后会忘记谁是主Chunk，它只需要等待60秒租约到期，那么它知道对于这个Chunk来说没有主Chunk，这个时候，Master节点可以安全指定一个新的主Chunk。所以这里标记成V。</li><li>类似的，租约过期时间也不用写入磁盘，所以这里标记成V。</li></ul><p>任何时候，如果文件扩展到达了一个新的64MB，需要新增一个Chunk或者由于指定了新的主Chunk而导致版本号更新了，Master节点需要向磁盘中的Log追加一条记录说，我刚刚向这个文件添加了一个新的Chunk或者我刚刚修改了Chunk的版本号。所以每次有这样的更新，都需要写磁盘。GFS论文并没有讨论这么多细节，但是因为写磁盘的速度是有限的，写磁盘会导致Master节点的更新速度也是有限的，所以要尽可能少的写入数据到磁盘。</p><h2 id="GFS读文件"><a href="#GFS读文件" class="headerlink" title="GFS读文件"></a>GFS读文件</h2><ol><li>客户端向master节点发送想要读取的文件名与想要读取数据在该文件中偏移量</li><li>Master节点会从自己的file表单中查询文件名，得到Chunk ID的数组。偏移量除以Chunk大小64MB就可以从数组中得到对应的Chunk ID。再从Chunk表单中找到存有Chunk的服务器列表，并将列表返回给客户端。</li><li>客户端从Chunk服务器中选一个网络上最近的服务器并向其发送读请求。同时，Chunk与服务器对应关系会被客户端缓存下来，再一次请求时则不用经过master节点。</li><li>Chunk服务器根据Chunk ID找到对应的Chunk文件，之后从文件中读取对应的数据段，并将数据返回给客户端。</li></ol><p class="note note-info">如果读取的数据超过了一个Chunk怎么办？<br><br>Robert教授：客户端本身依赖了一个GFS的库，这个库会注意到读请求跨越了Chunk的边界 ，并会将读请求拆分，之后再将它们合并起来。所以这个库会与Master节点交互，Master节点会告诉这个库说Chunk7在这个服务器，Chunk8在那个服务器。之后这个库会说，我需要Chunk7的最后两个字节，Chunk8的头两个字节。GFS库获取到这些数据之后，会将它们放在一个buffer中，再返回给调用库的应用程序。Master节点会告诉库有关Chunk的信息，而GFS库可以根据这个信息找到应用程序想要的数据。应用程序只需要确定文件名和数据在整个文件中的偏移量，GFS库和Master节点共同协商将这些信息转换成Chunk。</p><p class="note note-info">从哪个Chunk服务器读取数据重要吗？<br><br>Robert教授：是也不是。概念上讲，它们都是副本。实际上，你可能已经注意到，或者我们之前也说过，不同Chunk服务器上的数据并不一定完全相同。应用程序应该要能够容忍这种情况。所以，实际上，如果从不同的Chunk服务器读取数据，可能会略微不同。GFS论文提到，客户端会尝试从同一个机架或者同一个交换机上的服务器读取数据。</p><h2 id="GFS写文件"><a href="#GFS写文件" class="headerlink" title="GFS写文件"></a>GFS写文件</h2><h3 id="1-查找主副本"><a href="#1-查找主副本" class="headerlink" title="1. 查找主副本"></a>1. 查找主副本</h3><p>客户端向Master节点发送写请求并查询哪个Chunk服务器保存了文件的最后一个Chunk。</p><p>master找出chunk的主副本。如果发现Chunk的主副本不存在，Master会找出所有存有Chunk最新副本的Chunk服务器。（如果一个系统已经运行了很长时间，那么有可能某一个Chunk服务器保存的Chunk副本是旧的，比如说还是昨天或者上周的。导致这个现象的原因可能是服务器因为宕机而没有收到任何的更新。）最新的副本是指，副本中保存的版本号与Master中记录的Chunk的版本号一致。Chunk副本中的版本号是由Master节点下发的，所以Master节点知道，对于一个特定的Chunk，哪个版本号是最新的。</p><p class="note note-info">为什么不将所有Chunk服务器上保存的最大版本号作为Chunk的最新版本号？<br><br>Robert教授：当Master重启时，无论如何都需要与所有的Chunk服务器进行通信，因为Master需要确定哪个Chunk服务器存了哪个Chunk。你可能会想到，Master可以将所有Chunk服务器上的Chunk版本号汇总，找出里面的最大值作为最新的版本号。如果所有持有Chunk的服务器都响应了，那么这种方法是没有问题的。但是存在一种风险，当Master节点重启时，可能部分Chunk服务器离线或者失联或者自己也在重启，从而不能响应Master节点的请求。所以，Master节点可能只能获取到持有旧副本的Chunk服务器的响应，而持有最新副本的Chunk服务器还没有完成重启，或者还是离线状态（这个时候Master能找到的Chunk最大版本明显不对）。当Master找不到持有最新Chunk的服务器时该怎么办？Master节点会定期与Chunk服务器交互来查询它们持有什么样版本的Chunk。假设Master保存的Chunk版本是17，但是又没有找到存储了版本号是17的Chunk服务器，那么有两种可能：要么Master会等待，并不响应客户端的请求；要么会返回给客户端说，我现在还不知道Chunk在哪，过会再重试吧。比如说机房电源故障了，所有的服务器都崩溃了，我们正在缓慢的重启。Master节点和一些Chunk服务器可能可以先启动起来，一些Chunk服务器可能要5分钟以后才能重启，这种场景下，我们需要等待，甚至可能是永远等待，因为你不会想使用Chunk的旧数据。所以，总的来说，在重启时，因为Master从磁盘存储的数据知道Chunk对应的最新版本，Master节点会整合具有最新版本Chunk的服务器。每个Chunk服务器会记住本地存储Chunk对应的版本号，当Chunk服务器向Master汇报时，就可以说，我有这个Chunk的这个版本。而Master节点就可以忽略哪些版本号与已知版本不匹配的Chunk服务器。</p><h3 id="2-Primary与Secondary服务器"><a href="#2-Primary与Secondary服务器" class="headerlink" title="2. Primary与Secondary服务器"></a>2. Primary与Secondary服务器</h3><p>Master等待所有存储了最新Chunk版本的服务器集合完成，然后挑选一个作为Primary，其他的作为Secondary。Master增加版本号，并将版本号写入磁盘。Master节点会向Primary和Secondary副本对应的服务器发送消息并告诉它们，谁是Primary，谁是Secondary，Chunk的新版本是什么。Primary和Secondary服务器都会将版本号存储在本地的磁盘中。这样，当它们因为电源故障或者其他原因重启时，它们可以向Master报告本地保存的Chunk的实际版本号。</p><p>Primary接收来自客户端的写请求，并将写请求应用在多个Chunk服务器中。之所以要管理Chunk的版本号，是因为这样Master可以将实际更新Chunk的能力转移给Primary服务器。并且在将版本号更新到Primary和Secondary服务器之后，如果Master节点故障重启，还是可以在相同的Primary和Secondary服务器上继续更新Chunk。</p><p>Master节点通知Primary和Secondary服务器可以修改这个Chunk。它还给Primary一个租约，这个租约告诉Primary说，在接下来的60秒中，你将是Primary，60秒之后你必须停止成为Primary。这种机制可以确保不会同时有两个Primary。</p><p>假设现在Master节点告诉客户端谁是Primary，谁是Secondary，GFS提出了一种聪明的方法来实现写请求的执行序列。客户端会将要追加的数据发送给Primary和Secondary服务器，这些服务器会将数据写入到一个临时位置。所以最开始，这些数据不会追加到文件中。当所有的服务器都返回确认消息说，已经有了要追加的数据，客户端会向Primary服务器发送一条消息说，你和所有的Secondary服务器都有了要追加的数据，现在我想将这个数据追加到这个文件中。Primary服务器或许会从大量客户端收到大量的并发请求，Primary服务器会以某种顺序，一次只执行一个请求。对于每个客户端的追加数据请求（也就是写请求），Primary会查看当前文件结尾的Chunk，并确保Chunk中有足够的剩余空间，然后将客户端要追加的数据写入Chunk的末尾。并且，Primary会通知所有的Secondary服务器也将客户端要追加的数据写入在它们自己存储的Chunk末尾。这样，包括Primary在内的所有副本，都会收到通知将数据追加在Chunk的末尾。</p><p>但是对于Secondary服务器来说，它们可能可以执行成功，也可能会执行失败，比如说磁盘空间不足，比如说故障了，比如说Primary发出的消息网络丢包了。如果Secondary实际真的将数据写入到了本地磁盘存储的Chunk中，它会回复“yes”给Primary。如果所有的Secondary服务器都成功将数据写入，并将“yes”回复给了Primary，并且Primary也收到了这些回复。Primary会向客户端返回写入成功。如果至少一个Secondary服务器没有回复Primary，或者回复了，但是内容却是：抱歉，一些不好的事情发生了，比如说磁盘空间不够，或者磁盘故障了，Primary会向客户端返回写入失败。如果客户端从Primary得到写入失败，那么客户端应该重新发起整个追加过程。客户端首先会重新与Master交互，找到文件末尾的Chunk；之后，客户端需要重新发起对于Primary和Secondary的数据追加操作。</p><h2 id="GFS的一致性"><a href="#GFS的一致性" class="headerlink" title="GFS的一致性"></a>GFS的一致性</h2><p><img src="https://906337931-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-MAkokVMtbC7djI1pgSw%2F-MDlKCEjhSvXUjjI5-cA%2F-MDlMJ54nX5uNPISZ8vX%2Fimage.png?alt=media&token=9661caa0-ad11-4d50-892f-71f1ab069978" alt="B数据追加失败，再次追加后导致乱序"></p><p>GFS会出现乱序与未写入的情况。</p><h2 id="GFS的问题"><a href="#GFS的问题" class="headerlink" title="GFS的问题"></a>GFS的问题</h2><p>它最严重的局限可能在于，它只有一个Master节点，会带来以下问题：</p><ul><li>Master节点必须为每个文件，每个Chunk维护表单，随着GFS的应用越来越多，这意味着涉及的文件也越来越多，最终Master会耗尽内存来存储文件表单。你可以增加内存，但是单台计算机的内存也是有上限的。所以，这是人们遇到的最早的问题。</li><li>除此之外，单个Master节点要承载数千个客户端的请求，而Master节点的CPU每秒只能处理数百个请求，尤其Master还需要将部分数据写入磁盘，很快，客户端数量超过了单个Master的能力。</li><li>另一个问题是，应用程序发现很难处理GFS奇怪的语义（本节最开始介绍的GFS的副本数据的同步，或者可以说不同步）。</li><li>最后一个问题是，从我们读到的GFS论文中，Master节点的故障切换不是自动的。GFS需要人工干预来处理已经永久故障的Master节点，并更换新的服务器，这可能需要几十分钟甚至更长的而时间来处理。对于某些应用程序来说，这个时间太长了。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>MIT6.5840</tag>
      
      <tag>分布式系统</tag>
      
      <tag>GFS</tag>
      
      <tag>Note</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MIT 6.5840(6.824) | Lab1: MapReduce</title>
    <link href="/2023/08/07/MIT-6-5840-6-824-Lab1-MapReduce/"/>
    <url>/2023/08/07/MIT-6-5840-6-824-Lab1-MapReduce/</url>
    
    <content type="html"><![CDATA[<p>本篇文章主要讲解我的无显式锁MapReduce实现思路。</p><span id="more"></span><h2 id="开始之前"><a href="#开始之前" class="headerlink" title="开始之前"></a>开始之前</h2><p>在开始之前建议先熟悉Go语言并阅读过MapReduce的<a href="https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf">论文</a>或了解过MapReduce的原理。</p><p>使用IDE编写代码可以很方便的进行调试，这里使用Goland。关于项目的配置可以参考这篇<a href="https://zhuanlan.zhihu.com/p/260752052">知乎文章</a>（并不建议阅读他的代码，很显然作者是从Java语言转过来的，一些数据结构显得比较多余）。</p><p>无显式锁的思路主要受2021年Russ Cox的<a href="https://www.bilibili.com/video/BV16f4y1z7kn">guest lecture</a>启发, 其中的scheduler模式十分适用于实现MapReduce.</p><p>建议使用最新版本的<a href="https://pdos.csail.mit.edu/6.824/labs/lab-mr.html">代码模板</a>开始你的任务，相较于之前的散装代码，这个版本使用了gomod，并且减少了IDE中烦人的报错。同时，建议读者应该仔细阅读lab的rules与hint，对理解任务与代码编写都有很大的帮助。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="Task的定义"><a href="#Task的定义" class="headerlink" title="Task的定义"></a>Task的定义</h3><p>Task是需要通过RPC进行传递的，因此首先定义一下Task.</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> TaskType <span class="hljs-type">int</span><br><br><span class="hljs-keyword">const</span> (<br>MapState TaskType = <span class="hljs-literal">iota</span><br>ReduceState<br>WaitState<br>CompleteState<br>)<br><br><span class="hljs-keyword">type</span> TaskStatus <span class="hljs-type">int</span><br><br><span class="hljs-keyword">const</span> (<br>Init TaskStatus = <span class="hljs-literal">iota</span><br>Working<br>Complete<br>)<br><br><span class="hljs-keyword">type</span> Task <span class="hljs-keyword">struct</span> &#123;<br>FileName   <span class="hljs-type">string</span><br>TaskType   TaskType<br>TaskStatus TaskStatus<br>NReduce    <span class="hljs-type">int</span><br>FileId     <span class="hljs-type">int</span><br>StartTime  time.Time<br>&#125;<br></code></pre></td></tr></table></figure><p>相较于区分map任务与reduce任务，我认为两者共用一个struct即可，无需为两者做出区分。在编写代码时，你应该随着你的编码进度来修改task的结构，而不是直接照抄，这样会大大限制你的思考。</p><h3 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h3><p>Worker的实现比较简单，只需要对分配到的任务做出相应处理即可。注意，你应该有一个wait task，来应对一些需要worker等待的情况，例如hints里提到在开始reduce任务之前你需要等待map任务结束。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">Worker</span><span class="hljs-params">(mapf <span class="hljs-keyword">func</span>(<span class="hljs-type">string</span>, <span class="hljs-type">string</span>)</span></span> []KeyValue,<br>reducef <span class="hljs-function"><span class="hljs-keyword">func</span><span class="hljs-params">(<span class="hljs-type">string</span>, []<span class="hljs-type">string</span>)</span></span> <span class="hljs-type">string</span>) &#123;<br><br>task := AskTask(&amp;Task&#123;&#125;)<br><span class="hljs-keyword">for</span> &#123;<br><span class="hljs-keyword">switch</span> task.TaskType &#123;<br><span class="hljs-keyword">case</span> MapState:<br>task = DoMap(&amp;task, mapf)<br><span class="hljs-keyword">case</span> ReduceState:<br>task = DoReduce(&amp;task, reducef)<br><span class="hljs-keyword">case</span> WaitState:<br>fmt.Printf(<span class="hljs-string">&quot;wait for task\n&quot;</span>)<br>time.Sleep(<span class="hljs-number">2</span> * time.Second)<br>task = Task&#123;TaskType: WaitState&#125;<br><span class="hljs-keyword">case</span> CompleteState:<br><span class="hljs-keyword">return</span><br>&#125;<br>task = AskTask(&amp;task)<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>在中间文件的创建与读取中有一些细节需要注意，这里参考hints里提到的就好了。在worker完成当前任务并请求下一个任务的过程中，我将当前完成的任务作为参数发送给coordinator，方便coordinator对task进行状态维护（从workingChan转移至DoneChan）。</p><h3 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h3><p>在包括空行的情况下我仅仅用了133行代码实现了coordinator, 这主要归功于channel的使用与不维护worker状态的设计。</p><p>我对于Coordinator的定义如下：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs go"><span class="hljs-keyword">type</span> Coordinator <span class="hljs-keyword">struct</span> &#123;<br>taskChan    <span class="hljs-keyword">chan</span> Task<br>workingChan <span class="hljs-keyword">chan</span> Task<br>DoneChan    <span class="hljs-keyword">chan</span> Task<br>files       []<span class="hljs-type">string</span><br>phase       CoordinatorPhase<br>nReduce     <span class="hljs-type">int</span><br>&#125;<br></code></pre></td></tr></table></figure><p>可以看到我定义了三个channel，这次大规模使用channel让我体会到对于多线程分布式的任务，它的存在会简化我们的许多工作。<code>DoneChan</code>实际上有点多余，由于不需要知道<code>DoneChan</code>中task任务的具体信息，只需要知道task的数量，使用一个bool类型的数组也可以完成它的工作。</p><p>Coordinator的工作流程主要如下：</p><ol><li>将任务打入taskChan</li><li>接到worker任务请求，从workingChan取出worker发来的上一轮已完成任务并打入DoneChan</li><li>遍历workingChan查看是否有超时任务，如果超时则取出并放回taskChan供重新分配</li><li>若taskChan中还有任务，则从taskChan取出任务分配给worker后，将其打入workingChan</li><li>若taskChan中无任务，则分配waitTask</li></ol><p>可以发现只需要维护task的状态即可，并不需要关注worker的状态，如果worker没能完成这个任务，交给其他worker完成就可以了。</p><p>至于map阶段，reduce阶段与所有任务完成阶段的转换，只需要在每次call Done函数时进行判断，如果此时DoneChan中的任务数量与需要完成的数量相同，就可以判断此阶段完成，可以开启下一阶段了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>MIT6.5840</tag>
      
      <tag>分布式系统</tag>
      
      <tag>MapReduce</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
